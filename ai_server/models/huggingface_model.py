"""
í—ˆê¹…í˜ì´ìŠ¤ Meow-HyperCLOVAX í’€íŒŒì¸íŠœë‹ ëª¨ë¸ êµ¬í˜„ (T4 GPU ìµœì í™” ë²„ì „)
"""

import logging
import os
import torch
from typing import Optional
from transformers import AutoModelForCausalLM, AutoTokenizer
from ai_server.config import get_settings
# import wandb

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger(__name__)

class PostModel:
    """
    í’€íŒŒì¸íŠœë‹ëœ Meow-HyperCLOVAX ëª¨ë¸ì„ ë¡œë“œí•˜ê³  ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤ (T4 GPU ì „ìš©)
    """
    # í’€íŒŒì¸íŠœë‹ ëª¨ë¸ ê²½ë¡œ ë° íŒŒë¼ë¯¸í„° ì •ì˜
    FINE_TUNED_MODEL_PATH = "haebo/Meow-HyperCLOVAX-1.5B_FullFT_fp32_0527f"
    MODEL_LOAD_DTYPE = torch.float32
    MODEL_MAX_LENGTH = 1024 # HyperCLOVAX-SEED-1.5B(1/2 ê°’)
    MODEL_MAX_NEW_TOKENS = 200 
    MODEL_TEMPERATURE = 0.2
    MODEL_TOP_P = 0.3 # ì¢€ ë” ë‚®ì¶œ í•„ìš” ìˆì„ ë“¯ (ì‚¬ìš©í•  í™•ë¥  ë¶„í¬ê°’)
    MODEL_REPETITION_PENALTY = 1.6 # ë°˜ë³µ ì–µì œ 
    
    def __init__(self):
        """
        í’€íŒŒì¸íŠœë‹ ëª¨ë¸ ì´ˆê¸°í™”
        """
        # accelerate ê´€ë ¨ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (None ì˜¤ë¥˜ ë°©ì§€)
        os.environ.setdefault("ACCELERATE_USE_CPU", "false")
        os.environ.setdefault("ACCELERATE_USE_DEEPSPEED", "false")
        os.environ.setdefault("ACCELERATE_USE_FSDP", "false")
        
        # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
        self.fine_tuned_model_path = self.FINE_TUNED_MODEL_PATH
        self.model_load_dtype = self.MODEL_LOAD_DTYPE
        
        # ì¸ì¦ í† í°ì€ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜´
        settings = get_settings()
        self.auth_token = settings.HUGGINGFACE_TOKEN
        
        # GPU í™˜ê²½ ìµœì í™” ì„¤ì •
        self._setup_gpu()
        
        logger.info(f"íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ ì‹œì‘: {self.fine_tuned_model_path}")
        
        # í’€íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ
        self._load_fine_tuned_model()
        
        # ìƒì„± ì„¤ì • ì €ì¥
        self.max_new_tokens = self.MODEL_MAX_NEW_TOKENS
        self.temperature = self.MODEL_TEMPERATURE
        self.top_p = self.MODEL_TOP_P
        self.repetition_penalty = self.MODEL_REPETITION_PENALTY
        
        logger.info(f"íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {self.fine_tuned_model_path}")
        self._initialized = True

        # # W&B ì‹¤í—˜ ì¶”ì  ì´ˆê¸°í™”
        # wandb.init(
        #     project="meow-hyperclovax",
        #     name="FullFT-v0527-0611",  # ì‹¤í—˜ ì´ë¦„
        #     config={
        #         "model_path": self.FINE_TUNED_MODEL_PATH,
        #         "max_new_tokens": self.MODEL_MAX_NEW_TOKENS,
        #         "temperature": self.MODEL_TEMPERATURE,
        #         "top_p": self.MODEL_TOP_P,
        #         "repetition_penalty": self.MODEL_REPETITION_PENALTY,
        #         "dtype": str(self.MODEL_LOAD_DTYPE),
        #     }
        # )
    
    def _setup_gpu(self):
        """
        GPU ì„¤ì • ì´ˆê¸°í™” ë° ìµœì í™”
        """
        # GPU ë©”ëª¨ë¦¬ ë¶„í•  í¬ê¸° ìµœì í™”
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:128"
        
        # CUDA ë©”ëª¨ë¦¬ ìºì‹œ ì„¤ì •
        torch.cuda.empty_cache()
        
        # GPU ë©”ëª¨ë¦¬ ì •ë³´ ë¡œê¹…
        mem_total = torch.cuda.get_device_properties(0).total_memory / (1024**3)
        mem_name = torch.cuda.get_device_name(0)
        logger.info(f"GPU: {mem_name}, ë©”ëª¨ë¦¬: ì´ {mem_total:.2f}GB")
        logger.info("GPU ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _load_fine_tuned_model(self):
        """
        íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ì§ì ‘ ë¡œë“œ
        """
        # í’€íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì§ì ‘ ë¡œë“œ (32ë¹„íŠ¸)
        logger.info(f"íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œ ì¤‘: {self.fine_tuned_model_path}")
        
        self.model = AutoModelForCausalLM.from_pretrained(
            self.fine_tuned_model_path,
            device_map="auto",  # ìë™ ë””ë°”ì´ìŠ¤ ë§¤í•‘
            torch_dtype=self.model_load_dtype,  # 32ë¹„íŠ¸ ì •ë°€ë„
            token=self.auth_token,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        self.tokenizer = AutoTokenizer.from_pretrained(
            self.fine_tuned_model_path,
            token=self.auth_token,
            model_max_length=self.MODEL_MAX_LENGTH,
            trust_remote_code=True
        )
        
        # íŒ¨ë”© í† í° ì„¤ì • (í•„ìš”í•œ ê²½ìš°)
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # ëª¨ë¸ì„ í‰ê°€ ëª¨ë“œë¡œ ì„¤ì •
        self.model.eval()
        
        logger.info("í’€íŒŒì¸íŠœë‹ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì € ë¡œë“œ ì™„ë£Œ")
    
    def _postprocess(self, text: str, original_content: str = "") -> str:
        import re
        from collections import Counter

        EMOJI_PATTERN = (
            "[" +
            "\U0001F600-\U0001F64F"
            "\U0001F300-\U0001F5FF"
            "\U0001F680-\U0001F6FF"
            "\U0001F1E0-\U0001F1FF"
            "\U00002700-\U000027BF"
            "\U0001F900-\U0001F9FF"
            "\U00002600-\U000026FF"
            "]"
        )

        # transformed_content í¬ë§· ì œê±°
        text = re.sub(r"### transformed_content:\s*", "", text).strip()

        # ì¤„ë°”ê¿ˆ ë° ë°±ìŠ¬ë˜ì‹œ ì œê±°
        text = re.sub(r'(\\r\\n|\\r|\\n|\r|\n)', '', text)

        # í•´ì‹œíƒœê·¸ ì œê±°
        text = re.sub(r'#\S+', '', text)

        # ì´ëª¨ì§€ ì—°ì† ì œê±°
        text = re.sub(f"({EMOJI_PATTERN}){EMOJI_PATTERN}+", r"\1", text)

        # ì´ëª¨ì§€ 2ê°œ ì´ˆê³¼ ì‹œ ì•ì˜ 2ê°œë§Œ ë‚¨ê¸°ê³  ì œê±°
        emojis = re.findall(EMOJI_PATTERN, text)
        if len(emojis) > 2:
            keep = emojis[:2]
            text = re.sub(EMOJI_PATTERN, '', text) + ''.join(keep)

        # ê¸°í˜¸ë‚˜ ê°íƒ„ì‚¬ ë°˜ë³µ (ex. ! ! ! ...) ì••ì¶•
        text = re.sub(r'([!?\.ğŸ’¢â¤â­âœ¨ğŸ¾â€¦]{1})( \1|\1){2,}', r'\1\1', text)

        # íŠ¹ìˆ˜ë¬¸ì/ë¹„ì •ìƒ ë¬¸ì ì œê±°
        text = re.sub(r"[ï¸â€¹â€ºï¼]", '', text)

        # ë™ì¼ ë‹¨ì–´ ë°˜ë³µ ì¶•ì†Œ
        words = re.findall(r'\b\w+\b', text)
        counts = Counter(words)
        for word, count in counts.items():
            if count >= 4:
                text = re.sub(rf'\b({re.escape(word)})\b', '', text, count - 2)

        # ê¸ˆì§€ì–´ ì œê±°
        for word in ['system', 'ì•ˆì˜¬ë¼ê°„ë‹¤']:
            text = text.replace(word, '')

        # ë¹„ì •ìƒì ìœ¼ë¡œ ëŠê¸´ ë¬¸ì¥ ì •ë¦¬
        text = re.sub(r'([ê°€-í£a-zA-Z])\s*\.+\s*$', r'\1.', text)
        text = re.sub(r'\.\.+', '.', text)

        # ë„ˆë¬´ ì§§ê±°ë‚˜ ì˜ë¯¸ ì—†ëŠ” ë¬¸ì¥ ì œê±°
        if len(text) < 5 or re.fullmatch(r'[\W\d\s]+', text):
            return "[ì¶œë ¥ ì˜¤ë¥˜] ê²°ê³¼ ìƒì„±ì´ ì‹¤íŒ¨í–ˆì–´ìš”."

        # ê¸¸ì´ ì œí•œ
        if original_content:
            # ê¸¸ì´ ì¡°ê±´ ë³„ ìµœëŒ€ ê¸¸ì´ ì„¤ì •
            original_len = len(original_content)
            if original_len <= 30:
                max_len = int(3.0 * original_len)
            else: 
                max_len = int(2.0 * original_len)

            if len(text) > max_len:
                # ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ìë¥´ê³ , ì´ˆê³¼ ë¶€ë¶„ì„ ë²„ë¦¼
                words = text.split()
                trimmed_text = ""
                for word in words:
                    if len(trimmed_text) + len(word) + 1 > max_len:
                        break
                    trimmed_text += word + " "
                text = trimmed_text.strip()

        return re.sub(r'\s+', ' ', text).strip()


    async def generate(
        self, 
        prompt: str, 
        max_new_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        top_p: Optional[float] = None,
        repetition_penalty: Optional[float] = None,
        original_content: Optional[str] = str,
        **kwargs
    ) -> str:
        """
        í”„ë¡¬í”„íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ ìƒì„±
        
        Args:
            prompt: ì…ë ¥ í”„ë¡¬í”„íŠ¸
            max_new_tokens: ìµœëŒ€ ìƒì„± í† í° ìˆ˜
            temperature: ìƒì„± ì˜¨ë„ (ì°½ì˜ì„± ì¡°ì ˆ)
            top_p: nucleus sampling íŒŒë¼ë¯¸í„°
            **kwargs: ì¶”ê°€ ìƒì„± íŒŒë¼ë¯¸í„°
            
        Returns:
            ìƒì„±ëœ í…ìŠ¤íŠ¸
        """
        # ìƒì„± íŒŒë¼ë¯¸í„° ì„¤ì •
        # max_new_tokens = max_new_tokens or self.max_new_tokens
        temperature = temperature or self.temperature
        top_p = top_p or self.top_p
        repetition_penalty = repetition_penalty or self.repetition_penalty

        # ì…ë ¥ í† í°í™”
        inputs = self.tokenizer.encode(prompt, return_tensors="pt").to(self.model.device)
        max_new_tokens = max_new_tokens or self.max_new_tokens
        
        # ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„±
        attention_mask = torch.ones_like(inputs)
        # í…ìŠ¤íŠ¸ ìƒì„±
        with torch.no_grad():
            outputs = self.model.generate(
                inputs,
                attention_mask=attention_mask,
                max_new_tokens=max_new_tokens,
                temperature=temperature, # ë‚®ê²Œ ì„¤ì • : ë†’ì€ í™•ë¥ ì˜ í† í° ìœ„ì£¼ ì„ íƒ
                do_sample=True, # True + ë‚®ì€ Temp : ì•½ê°„ì˜ ë‹¤ì–‘ì„± ë¶€ì—¬ (False í•˜ë©´ ê°€ì¥ ë†’ì€ í™•ë¥ ì˜ ê²°ê³¼ë§Œ ë„ì¶œ)
                top_p=top_p,
                pad_token_id=self.tokenizer.eos_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                repetition_penalty=repetition_penalty,  
                **kwargs
            )

        # ìƒˆë¡œ ìƒì„±ëœ í† í°ë§Œ ë””ì½”ë”© (ì…ë ¥ ì œì™¸)
        new_tokens = outputs[0][inputs.shape[1]:]
        decoded = self.tokenizer.decode(new_tokens, skip_special_tokens=True)
        processed_text = self._postprocess(decoded, original_content)

        # # W&B ë¡œê·¸ ê¸°ë¡
        # wandb.log({
        #     "prompt": prompt,
        #     "generated_text": processed_text,
        #     "output_length": len(self.tokenizer.encode(processed_text)),
        #     "temperature": temperature,
        #     "top_p": top_p,
        #     "repetition_penalty": repetition_penalty
        # })
        # print("processed_text:", processed_text)
        return processed_text
    
    def get_model_info(self) -> dict:
        """
        ëª¨ë¸ ì •ë³´ ë°˜í™˜
        
        Returns:
            ëª¨ë¸ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        """
        return {
            "model_path": self.fine_tuned_model_path,
            "max_length": self.model_max_length,
            "max_new_tokens": self.max_new_tokens,
            "temperature": self.temperature,
            "top_p": self.top_p,
            "device": str(self.model.device) if hasattr(self, 'model') else "not_loaded",
            "initialized": getattr(self, '_initialized', False),
            "repetition_penalty" : self.repetition_penalty,
        }
    
    def unload(self):
        """ëª¨ë¸ ë©”ëª¨ë¦¬ì—ì„œ ì–¸ë¡œë“œ"""
        if hasattr(self, "model"):
            del self.model
            del self.tokenizer
            self._initialized = False
            torch.cuda.empty_cache()
            logger.info("í’€íŒŒì¸íŠœë‹ ëª¨ë¸ ì–¸ë¡œë“œ ì™„ë£Œ") 