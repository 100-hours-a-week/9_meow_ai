"""
vLLM ì„œë²„ ëŸ°ì²˜
ë‹¤ì¤‘ ëª¨ë¸ íƒ€ì… ì§€ì› (LoRA, í’€ íŒŒì¸íŠœë‹)
"""

import os
import sys
import subprocess
import signal
import time
import logging
from typing import Optional
from pathlib import Path
import requests
from huggingface_hub import HfApi
from huggingface_hub.utils import RepositoryNotFoundError

from .vllm_config import VLLMConfig, VLLMServerArgs, get_vllm_config, ModelType


# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ModelDetector:
    """ëª¨ë¸ íƒ€ì… ìë™ ê°ì§€ í´ë˜ìŠ¤"""
    
    @staticmethod
    def detect_model_type(model_path: str) -> ModelType:
        """ëª¨ë¸ ê²½ë¡œë¡œë¶€í„° ëª¨ë¸ íƒ€ì… ê°ì§€"""
        try:
            # í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ì¸ ê²½ìš°
            if "/" in model_path and not model_path.startswith(("./", "/")):
                return ModelDetector._detect_hf_model_type(model_path)
            
            # ë¡œì»¬ ëª¨ë¸ì¸ ê²½ìš°
            return ModelDetector._detect_local_model_type(model_path)
            
        except Exception as e:
            logger.warning(f"ëª¨ë¸ íƒ€ì… ê°ì§€ ì‹¤íŒ¨: {e}")
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_path}")
    
    @staticmethod
    def _detect_hf_model_type(model_path: str) -> ModelType:
        """í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ íƒ€ì… ê°ì§€"""
        try:
            api = HfApi()
            
            # ëª¨ë¸ ì •ë³´ ì¡°íšŒ
            model_info = api.model_info(model_path)
            
            # íƒœê·¸ ê¸°ë°˜ ê°ì§€
            if model_info.tags:
                tags = [tag.lower() for tag in model_info.tags]
                if "lora" in tags or "peft" in tags:
                    return ModelType.LORA
                elif "fine-tuned" in tags or "finetuned" in tags:
                    return ModelType.FULL_FINETUNED
            
            # íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ ê°ì§€
            try:
                files = api.list_repo_files(model_path)
                
                # LoRA ê´€ë ¨ íŒŒì¼ í™•ì¸
                lora_files = [
                    "adapter_config.json", 
                    "adapter_model.bin", 
                    "adapter_model.safetensors",
                    "peft_config.json"
                ]
                
                if any(f in files for f in lora_files):
                    return ModelType.LORA
                
                # í’€ ëª¨ë¸ íŒŒì¼ í™•ì¸
                full_model_files = [
                    "pytorch_model.bin",
                    "model.safetensors", 
                    "pytorch_model-00001-of-*.bin"
                ]
                
                if any(any(f.startswith(pattern.split('*')[0]) for f in files) 
                       for pattern in full_model_files):
                    return ModelType.FULL_FINETUNED
                    
            except Exception as e:
                logger.debug(f"íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            
            # ëª¨ë¸ëª… ê¸°ë°˜ ì¶”ë¡ 
            model_name_lower = model_path.lower()
            if "lora" in model_name_lower or "peft" in model_name_lower:
                return ModelType.LORA
            elif "ft" in model_name_lower or "finetun" in model_name_lower:
                return ModelType.FULL_FINETUNED
                
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_path}")
            
        except RepositoryNotFoundError:
            logger.error(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
            raise ValueError(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        except Exception as e:
            logger.error(f"í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ê°ì§€ ì‹¤íŒ¨: {e}")
            raise ValueError(f"ëª¨ë¸ ê°ì§€ ì‹¤íŒ¨: {model_path}")
    
    @staticmethod
    def _detect_local_model_type(model_path: str) -> ModelType:
        """ë¡œì»¬ ëª¨ë¸ íƒ€ì… ê°ì§€"""
        path = Path(model_path)
        
        if not path.exists():
            logger.warning(f"ë¡œì»¬ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
            raise ValueError(f"ë¡œì»¬ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
        
        # LoRA ê´€ë ¨ íŒŒì¼ í™•ì¸
        lora_files = [
            "adapter_config.json",
            "adapter_model.bin", 
            "adapter_model.safetensors",
            "peft_config.json"
        ]
        
        if any((path / f).exists() for f in lora_files):
            return ModelType.LORA
        
        # í’€ ëª¨ë¸ íŒŒì¼ í™•ì¸
        full_model_files = [
            "pytorch_model.bin",
            "model.safetensors"
        ]
        
        if any((path / f).exists() for f in full_model_files):
            return ModelType.FULL_FINETUNED
        
        raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_path}")


class VLLMLauncher:
    """vLLM ì„œë²„ ëŸ°ì²˜ í´ë˜ìŠ¤ - ë‹¤ì¤‘ ëª¨ë¸ íƒ€ì… ì§€ì›"""
    
    def __init__(self, config: Optional[VLLMConfig] = None):
        self.config = config or get_vllm_config()
        self.process: Optional[subprocess.Popen] = None
        self.server_args = VLLMServerArgs(self.config)
        self.detector = ModelDetector()
    
    def start_server(self) -> bool:
        """vLLM ì„œë²„ ì‹œì‘ - ëª¨ë¸ íƒ€ì…ë³„ ìµœì í™”"""
        if self.process and self.process.poll() is None:
            logger.warning("vLLM ì„œë²„ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            return True
        
        # ì„œë²„ ì‹¤í–‰ ëª…ë ¹ êµ¬ì„±
        cmd = ["python", "-m", "vllm.entrypoints.openai.api_server"] + self.server_args.get_server_args()
        
        model_config = self.config.get_current_model_config()
        logger.info(f"ğŸš€ {model_config.model_type.value} ëª¨ë¸ ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        logger.info(f"ëª¨ë¸: {self.config.active_model}")
        logger.info(f"ì‹¤í–‰ ëª…ë ¹: {' '.join(cmd)}")
        
        try:
            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            env = os.environ.copy()
            env.update({
                "CUDA_VISIBLE_DEVICES": "0",
                "VLLM_WORKER_MULTIPROC_METHOD": "spawn",  # ì•ˆì •ì„± í–¥ìƒ
            })
            
            self.process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                env=env,
                preexec_fn=os.setsid
            )
            
            logger.info(f"vLLM ì„œë²„ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. PID: {self.process.pid}")
            return self._wait_for_server_ready()
            
        except Exception as e:
            logger.error(f"vLLM ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
            return False
    
    def _wait_for_server_ready(self, timeout: int = 300) -> bool:
        """ì„œë²„ ì¤€ë¹„ ìƒíƒœ ëŒ€ê¸° - ëª¨ë¸ íƒ€ì…ë³„ ì¡°ì •ëœ íƒ€ì„ì•„ì›ƒ"""
        url = f"http://{self.config.host}:{self.config.port}/v1/models"
        start_time = time.time()
        
        model_config = self.config.get_current_model_config()
        
        # ëª¨ë¸ íƒ€ì…ë³„ íƒ€ì„ì•„ì›ƒ ì¡°ì •
        if model_config.model_type == ModelType.LORA:
            timeout = 240  # LoRAëŠ” ë² ì´ìŠ¤ ëª¨ë¸ + ì–´ëŒ‘í„° ë¡œë”© ì‹œê°„
        elif model_config.model_type == ModelType.FULL_FINETUNED:
            timeout = 360  # í’€ íŒŒì¸íŠœë‹ ëª¨ë¸ì€ ë” ê¸´ ë¡œë”© ì‹œê°„
        
        logger.info(f"ì„œë²„ ì¤€ë¹„ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤... (ìµœëŒ€ {timeout}ì´ˆ)")
        
        while time.time() - start_time < timeout:
            if self.process and self.process.poll() is not None:
                # í”„ë¡œì„¸ìŠ¤ ì˜¤ë¥˜ ë¡œê·¸ ì¶œë ¥
                stdout, stderr = self.process.communicate()
                logger.error("vLLM ì„œë²„ í”„ë¡œì„¸ìŠ¤ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                if stderr:
                    logger.error(f"ì˜¤ë¥˜ ë¡œê·¸: {stderr.decode()}")
                return False
            
            try:
                response = requests.get(url, timeout=5)
                if response.status_code == 200:
                    logger.info(f"ğŸ‰ {model_config.model_type.value} ëª¨ë¸ ì„œë²„ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    logger.info(f"ì„œë¹™ ëª¨ë¸ëª…: {model_config.served_model_name}")
                    return True
            except requests.exceptions.RequestException:
                pass
            
            time.sleep(3)
        
        logger.error(f"ì„œë²„ ì¤€ë¹„ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼ ({timeout}ì´ˆ)")
        return False
    
    def stop_server(self) -> bool:
        """vLLM ì„œë²„ ì¤‘ì§€"""
        if not self.process:
            logger.info("ì‹¤í–‰ ì¤‘ì¸ vLLM ì„œë²„ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return True
        
        if self.process.poll() is not None:
            logger.info("vLLM ì„œë²„ê°€ ì´ë¯¸ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
        
        logger.info("vLLM ì„œë²„ë¥¼ ì¤‘ì§€í•©ë‹ˆë‹¤...")
        
        try:
            os.killpg(os.getpgid(self.process.pid), signal.SIGTERM)
            
            try:
                self.process.wait(timeout=30)
                logger.info("vLLM ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return True
            except subprocess.TimeoutExpired:
                logger.warning("ì •ìƒ ì¢…ë£Œ ì‹œê°„ ì´ˆê³¼. ê°•ì œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                os.killpg(os.getpgid(self.process.pid), signal.SIGKILL)
                self.process.wait()
                return True
                
        except Exception as e:
            logger.error(f"ì„œë²„ ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False
        finally:
            self.process = None
    
    def restart_server(self) -> bool:
        """vLLM ì„œë²„ ì¬ì‹œì‘"""
        logger.info("vLLM ì„œë²„ë¥¼ ì¬ì‹œì‘í•©ë‹ˆë‹¤...")
        self.stop_server()
        time.sleep(5)
        return self.start_server()
    
    def get_server_status(self) -> dict:
        """ì„œë²„ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        model_config = self.config.get_current_model_config()
        
        status = {
            "running": self.process is not None and self.process.poll() is None,
            "pid": self.process.pid if self.process else None,
            "active_model": self.config.active_model,
            "model_type": model_config.model_type.value,
            "served_model_name": model_config.served_model_name,
            "host": self.config.host,
            "port": self.config.port
        }
        
        return status


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ë‹¤ì¤‘ ëª¨ë¸ íƒ€ì… ì§€ì› vLLM ì„œë²„ ëŸ°ì²˜")
    parser.add_argument("--action", choices=["start", "stop", "restart", "status"], 
                       default="start", help="ì‹¤í–‰í•  ì•¡ì…˜")
    parser.add_argument("--port", type=int, help="ì„œë²„ í¬íŠ¸")
    
    args = parser.parse_args()
    
    launcher = VLLMLauncher()
    
    if args.action == "start":
        success = launcher.start_server()
        sys.exit(0 if success else 1)
    elif args.action == "stop":
        success = launcher.stop_server()
        sys.exit(0 if success else 1)
    elif args.action == "restart":
        success = launcher.restart_server()
        sys.exit(0 if success else 1)
    elif args.action == "status":
        status = launcher.get_server_status()
        print("=== vLLM ì„œë²„ ìƒíƒœ ===")
        for key, value in status.items():
            print(f"{key}: {value}")
        sys.exit(0)


if __name__ == "__main__":
    main() 