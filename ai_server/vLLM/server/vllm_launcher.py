"""
vLLM ì„œë²„ ëŸ°ì²˜
ë‹¤ì¤‘ ëª¨ë¸ íƒ€ì… ì§€ì› (LoRA, í’€ íŒŒì¸íŠœë‹)
"""

import os
import sys
import subprocess
import signal
import time
import logging
from typing import Optional
from pathlib import Path
import requests
from huggingface_hub import HfApi, hf_hub_download
from huggingface_hub.utils import RepositoryNotFoundError

from .vllm_config import VLLMConfig, VLLMServerArgs, get_vllm_config, ModelType, switch_model


# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ModelDetector:
    """ëª¨ë¸ íƒ€ì… ìë™ ê°ì§€ í´ë˜ìŠ¤"""
    
    @staticmethod
    def detect_model_type(model_path: str) -> ModelType:
        """ëª¨ë¸ ê²½ë¡œë¡œë¶€í„° ëª¨ë¸ íƒ€ì… ê°ì§€"""
        try:
            # í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ì¸ ê²½ìš°
            if "/" in model_path and not model_path.startswith(("./", "/")):
                return ModelDetector._detect_hf_model_type(model_path)
            
            # ë¡œì»¬ ëª¨ë¸ì¸ ê²½ìš°
            return ModelDetector._detect_local_model_type(model_path)
            
        except Exception as e:
            logger.warning(f"ëª¨ë¸ íƒ€ì… ê°ì§€ ì‹¤íŒ¨: {e}")
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_path}")
    
    @staticmethod
    def _detect_hf_model_type(model_path: str) -> ModelType:
        """í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ íƒ€ì… ê°ì§€"""
        try:
            api = HfApi()
            
            # ëª¨ë¸ ì •ë³´ ì¡°íšŒ
            model_info = api.model_info(model_path)
            
            # íƒœê·¸ ê¸°ë°˜ ê°ì§€
            if model_info.tags:
                tags = [tag.lower() for tag in model_info.tags]
                if "lora" in tags or "peft" in tags:
                    return ModelType.LORA
                elif "fine-tuned" in tags or "finetuned" in tags:
                    return ModelType.FULL_FINETUNED
            
            # íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ ê°ì§€
            try:
                files = api.list_repo_files(model_path)
                
                # LoRA ê´€ë ¨ íŒŒì¼ í™•ì¸
                lora_files = [
                    "adapter_config.json", 
                    "adapter_model.bin", 
                    "adapter_model.safetensors",
                    "peft_config.json"
                ]
                
                if any(f in files for f in lora_files):
                    return ModelType.LORA
                
                # í’€ ëª¨ë¸ íŒŒì¼ í™•ì¸
                full_model_files = [
                    "pytorch_model.bin",
                    "model.safetensors", 
                    "pytorch_model-00001-of-*.bin"
                ]
                
                if any(any(f.startswith(pattern.split('*')[0]) for f in files) 
                       for pattern in full_model_files):
                    return ModelType.FULL_FINETUNED
                    
            except Exception as e:
                logger.debug(f"íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            
            # ëª¨ë¸ëª… ê¸°ë°˜ ì¶”ë¡ 
            model_name_lower = model_path.lower()
            if "lora" in model_name_lower or "peft" in model_name_lower:
                return ModelType.LORA
            elif "ft" in model_name_lower or "finetun" in model_name_lower:
                return ModelType.FULL_FINETUNED
                
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_path}")
            
        except RepositoryNotFoundError:
            logger.error(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
            raise ValueError(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        except Exception as e:
            logger.error(f"í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ê°ì§€ ì‹¤íŒ¨: {e}")
            raise ValueError(f"ëª¨ë¸ ê°ì§€ ì‹¤íŒ¨: {model_path}")
    
    @staticmethod
    def _detect_local_model_type(model_path: str) -> ModelType:
        """ë¡œì»¬ ëª¨ë¸ íƒ€ì… ê°ì§€"""
        path = Path(model_path)
        
        if not path.exists():
            logger.warning(f"ë¡œì»¬ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
            raise ValueError(f"ë¡œì»¬ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
        
        # LoRA ê´€ë ¨ íŒŒì¼ í™•ì¸
        lora_files = [
            "adapter_config.json",
            "adapter_model.bin", 
            "adapter_model.safetensors",
            "peft_config.json"
        ]
        
        if any((path / f).exists() for f in lora_files):
            return ModelType.LORA
        
        # í’€ ëª¨ë¸ íŒŒì¼ í™•ì¸
        full_model_files = [
            "pytorch_model.bin",
            "model.safetensors"
        ]
        
        if any((path / f).exists() for f in full_model_files):
            return ModelType.FULL_FINETUNED
        
        raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_path}")


class VLLMLauncher:
    """vLLM ì„œë²„ ëŸ°ì²˜ í´ë˜ìŠ¤ - ë‹¤ì¤‘ ëª¨ë¸ íƒ€ì… ì§€ì›"""
    
    def __init__(self, config: Optional[VLLMConfig] = None):
        self.config = config or get_vllm_config()
        self.process: Optional[subprocess.Popen] = None
        self.server_args = VLLMServerArgs(self.config)
        self.detector = ModelDetector()
        
    def validate_model_setup(self) -> bool:
        """í˜„ì¬ ëª¨ë¸ ì„¤ì • ìœ íš¨ì„± ê²€ì‚¬"""
        model_config = self.config.get_current_model_config()
        
        logger.info(f"ëª¨ë¸ íƒ€ì…: {model_config.model_type}")
        logger.info(f"ëª¨ë¸ ê²½ë¡œ: {model_config.model_path}")
        
        if model_config.model_type == ModelType.LORA:
            return self._validate_lora_model(model_config)
        elif model_config.model_type == ModelType.FULL_FINETUNED:
            return self._validate_full_finetuned_model(model_config)
        else:
            logger.error(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_config.model_type}")
            return False
    
    def _validate_lora_model(self, model_config) -> bool:
        """LoRA ëª¨ë¸ ê²€ì¦"""
        # ë² ì´ìŠ¤ ëª¨ë¸ ê²½ë¡œ í™•ì¸
        if not model_config.base_model_path:
            logger.error("LoRA ëª¨ë¸ì—ëŠ” base_model_pathê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            return False
        
        # ë² ì´ìŠ¤ ëª¨ë¸ ì¡´ì¬ í™•ì¸
        if not self._check_model_exists(model_config.base_model_path):
            logger.error(f"ë² ì´ìŠ¤ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_config.base_model_path}")
            return False
        
        # LoRA ì–´ëŒ‘í„° í™•ì¸
        if not self._check_model_exists(model_config.model_path):
            logger.error(f"LoRA ì–´ëŒ‘í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_config.model_path}")
            return False
        
        logger.info("âœ… LoRA ëª¨ë¸ ì„¤ì •ì´ ìœ íš¨í•©ë‹ˆë‹¤.")
        return True
    
    def _validate_full_finetuned_model(self, model_config) -> bool:
        """í’€ íŒŒì¸íŠœë‹ ëª¨ë¸ ê²€ì¦"""
        if not self._check_model_exists(model_config.model_path):
            logger.error(f"í’€ íŒŒì¸íŠœë‹ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_config.model_path}")
            return False
        
        logger.info("âœ… í’€ íŒŒì¸íŠœë‹ ëª¨ë¸ ì„¤ì •ì´ ìœ íš¨í•©ë‹ˆë‹¤.")
        return True
    
    def _check_model_exists(self, model_path: str) -> bool:
        """ëª¨ë¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        # í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ì´ë¦„ í˜•ì‹ ì²´í¬
        if "/" in model_path and not model_path.startswith(("./", "/")):
            try:
                api = HfApi()
                api.model_info(model_path)
                logger.info(f"í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ í™•ì¸: {model_path}")
                return True
            except RepositoryNotFoundError:
                return False
            except Exception as e:
                logger.warning(f"ëª¨ë¸ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
                return False
        
        # ë¡œì»¬ ê²½ë¡œ ì²´í¬
        model_path_obj = Path(model_path)
        if model_path_obj.exists():
            logger.info(f"ë¡œì»¬ ëª¨ë¸ í™•ì¸: {model_path}")
            return True
        
        return False
    
    def switch_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ì „í™˜"""
        try:
            # í˜„ì¬ ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ë©´ ì¤‘ì§€
            if self.process and self.process.poll() is None:
                logger.info("ê¸°ì¡´ ì„œë²„ë¥¼ ì¤‘ì§€í•©ë‹ˆë‹¤...")
                self.stop_server()
                time.sleep(2)
            
            # ëª¨ë¸ ì „í™˜
            switch_model(model_name)
            self.config = get_vllm_config()
            self.server_args = VLLMServerArgs(self.config)
            
            logger.info(f"ëª¨ë¸ì´ '{model_name}'ìœ¼ë¡œ ì „í™˜ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì „í™˜ ì‹¤íŒ¨: {e}")
            return False
    
    def auto_detect_and_configure_model(self, model_path: str) -> bool:
        """ëª¨ë¸ ìë™ ê°ì§€ ë° ì„¤ì •"""
        try:
            detected_type = self.detector.detect_model_type(model_path)
            logger.info(f"ê°ì§€ëœ ëª¨ë¸ íƒ€ì…: {detected_type}")
            
            # TODO: ê°ì§€ëœ íƒ€ì…ì— ë”°ë¼ ì„¤ì • ìë™ ì—…ë°ì´íŠ¸
            # í˜„ì¬ëŠ” ìˆ˜ë™ìœ¼ë¡œ ì„¤ì •ëœ ëª¨ë¸ë§Œ ì§€ì›
            
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ìë™ ê°ì§€ ì‹¤íŒ¨: {e}")
            return False
    
    def start_server(self) -> bool:
        """vLLM ì„œë²„ ì‹œì‘ - ëª¨ë¸ íƒ€ì…ë³„ ìµœì í™”"""
        if self.process and self.process.poll() is None:
            logger.warning("vLLM ì„œë²„ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            return True
        
        if not self.validate_model_setup():
            logger.error("ëª¨ë¸ ì„¤ì • ê²€ì¦ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            return False
        
        # ì„œë²„ ì‹¤í–‰ ëª…ë ¹ êµ¬ì„±
        cmd = ["python", "-m", "vllm.entrypoints.openai.api_server"] + self.server_args.get_server_args()
        
        model_config = self.config.get_current_model_config()
        logger.info(f"ğŸš€ {model_config.model_type.value} ëª¨ë¸ ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        logger.info(f"ëª¨ë¸: {self.config.active_model}")
        logger.info(f"ì‹¤í–‰ ëª…ë ¹: {' '.join(cmd)}")
        
        try:
            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            env = os.environ.copy()
            env.update({
                "CUDA_VISIBLE_DEVICES": "0",
                "VLLM_WORKER_MULTIPROC_METHOD": "spawn",  # ì•ˆì •ì„± í–¥ìƒ
            })
            
            self.process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                env=env,
                preexec_fn=os.setsid
            )
            
            logger.info(f"vLLM ì„œë²„ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. PID: {self.process.pid}")
            return self._wait_for_server_ready()
            
        except Exception as e:
            logger.error(f"vLLM ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
            return False
    
    def _wait_for_server_ready(self, timeout: int = 300) -> bool:
        """ì„œë²„ ì¤€ë¹„ ìƒíƒœ ëŒ€ê¸° - ëª¨ë¸ íƒ€ì…ë³„ ì¡°ì •ëœ íƒ€ì„ì•„ì›ƒ"""
        url = f"http://{self.config.host}:{self.config.port}/v1/models"
        start_time = time.time()
        
        model_config = self.config.get_current_model_config()
        
        # ëª¨ë¸ íƒ€ì…ë³„ íƒ€ì„ì•„ì›ƒ ì¡°ì •
        if model_config.model_type == ModelType.LORA:
            timeout = 240  # LoRAëŠ” ë² ì´ìŠ¤ ëª¨ë¸ + ì–´ëŒ‘í„° ë¡œë”© ì‹œê°„
        elif model_config.model_type == ModelType.FULL_FINETUNED:
            timeout = 360  # í’€ íŒŒì¸íŠœë‹ ëª¨ë¸ì€ ë” ê¸´ ë¡œë”© ì‹œê°„
        
        logger.info(f"ì„œë²„ ì¤€ë¹„ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤... (ìµœëŒ€ {timeout}ì´ˆ)")
        
        while time.time() - start_time < timeout:
            if self.process and self.process.poll() is not None:
                # í”„ë¡œì„¸ìŠ¤ ì˜¤ë¥˜ ë¡œê·¸ ì¶œë ¥
                stdout, stderr = self.process.communicate()
                logger.error("vLLM ì„œë²„ í”„ë¡œì„¸ìŠ¤ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                if stderr:
                    logger.error(f"ì˜¤ë¥˜ ë¡œê·¸: {stderr.decode()}")
                return False
            
            try:
                response = requests.get(url, timeout=5)
                if response.status_code == 200:
                    logger.info(f"ğŸ‰ {model_config.model_type.value} ëª¨ë¸ ì„œë²„ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    logger.info(f"ì„œë¹™ ëª¨ë¸ëª…: {model_config.served_model_name}")
                    return True
            except requests.exceptions.RequestException:
                pass
            
            time.sleep(3)
        
        logger.error(f"ì„œë²„ ì¤€ë¹„ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼ ({timeout}ì´ˆ)")
        return False
    
    def stop_server(self) -> bool:
        """vLLM ì„œë²„ ì¤‘ì§€"""
        if not self.process:
            logger.info("ì‹¤í–‰ ì¤‘ì¸ vLLM ì„œë²„ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return True
        
        if self.process.poll() is not None:
            logger.info("vLLM ì„œë²„ê°€ ì´ë¯¸ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
        
        logger.info("vLLM ì„œë²„ë¥¼ ì¤‘ì§€í•©ë‹ˆë‹¤...")
        
        try:
            os.killpg(os.getpgid(self.process.pid), signal.SIGTERM)
            
            try:
                self.process.wait(timeout=30)
                logger.info("vLLM ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                return True
            except subprocess.TimeoutExpired:
                logger.warning("ì •ìƒ ì¢…ë£Œ ì‹œê°„ ì´ˆê³¼. ê°•ì œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                os.killpg(os.getpgid(self.process.pid), signal.SIGKILL)
                self.process.wait()
                return True
                
        except Exception as e:
            logger.error(f"ì„œë²„ ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False
        finally:
            self.process = None
    
    def restart_server(self) -> bool:
        """vLLM ì„œë²„ ì¬ì‹œì‘"""
        logger.info("vLLM ì„œë²„ë¥¼ ì¬ì‹œì‘í•©ë‹ˆë‹¤...")
        self.stop_server()
        time.sleep(5)
        return self.start_server()
    
    def get_server_status(self) -> dict:
        """ì„œë²„ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        model_config = self.config.get_current_model_config()
        
        status = {
            "running": self.process is not None and self.process.poll() is None,
            "pid": self.process.pid if self.process else None,
            "active_model": self.config.active_model,
            "model_type": model_config.model_type.value,
            "served_model_name": model_config.served_model_name,
            "host": self.config.host,
            "port": self.config.port
        }
        
        return status


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ë‹¤ì¤‘ ëª¨ë¸ íƒ€ì… ì§€ì› vLLM ì„œë²„ ëŸ°ì²˜")
    parser.add_argument("--action", choices=["start", "stop", "restart", "switch", "status"], 
                       default="start", help="ì‹¤í–‰í•  ì•¡ì…˜")
    parser.add_argument("--model", help="ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„ (switch ì•¡ì…˜ìš©)")
    parser.add_argument("--port", type=int, help="ì„œë²„ í¬íŠ¸")
    
    args = parser.parse_args()
    
    launcher = VLLMLauncher()
    
    if args.action == "start":
        success = launcher.start_server()
        sys.exit(0 if success else 1)
    elif args.action == "stop":
        success = launcher.stop_server()
        sys.exit(0 if success else 1)
    elif args.action == "restart":
        success = launcher.restart_server()
        sys.exit(0 if success else 1)
    elif args.action == "switch":
        if not args.model:
            logger.error("--model ì¸ìê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            sys.exit(1)
        success = launcher.switch_model(args.model)
        if success:
            success = launcher.start_server()
        sys.exit(0 if success else 1)
    elif args.action == "status":
        status = launcher.get_server_status()
        print("=== vLLM ì„œë²„ ìƒíƒœ ===")
        for key, value in status.items():
            print(f"{key}: {value}")
        sys.exit(0)


if __name__ == "__main__":
    main() 