"""
vLLM ì„œë²„ ì„¤ì • ê´€ë¦¬
í¬ìŠ¤íŠ¸ ë¬¸ì¥ ìƒì„±ì„ ìœ„í•œ ê°„ì†Œí™”ëœ ì„¤ì •
"""

import os
from typing import Optional, List, Dict, Any
from enum import Enum
from pydantic import BaseModel, Field
from pydantic_settings import BaseSettings
import httpx
import logging

logger = logging.getLogger(__name__)


class ModelType(str, Enum):
    """ëª¨ë¸ íƒ€ì… ì •ì˜"""
    LORA = "lora"
    FULL_FINETUNED = "full_finetuned"


class ModelConfig(BaseModel):
    """ê°œë³„ ëª¨ë¸ ì„¤ì •"""
    model_type: ModelType
    model_path: str
    base_model_path: Optional[str] = None
    lora_modules: Optional[List[str]] = None
    gpu_memory_utilization: float = 0.8
    max_model_len: int = 1024
    max_num_batched_tokens: int = 2048
    max_num_seqs: int = 32
    served_model_name: Optional[str] = None


def get_default_models() -> Dict[str, ModelConfig]:
    """ê¸°ë³¸ ì§€ì› ëª¨ë¸ ì„¤ì • ë°˜í™˜"""
    return {
        "haebo/Meow-HyperCLOVAX-1.5B_LoRA_fp16_0619i": ModelConfig(
            model_type=ModelType.LORA,
            model_path="haebo/Meow-HyperCLOVAX-1.5B_LoRA_fp16_0619i",
            base_model_path="naver-hyperclovax/HyperCLOVAX-SEED-Text-Instruct-1.5B",
            lora_modules=["lora=haebo/Meow-HyperCLOVAX-1.5B_LoRA_fp16_0619i"],
            gpu_memory_utilization=0.6,   # ì•ˆì •ì„± ì¤‘ì‹¬, ë©”ëª¨ë¦¬ ì—¬ìœ  í™•ë³´
            max_model_len=1536,           # í•œêµ­ì–´ 600ì ì²˜ë¦¬ 
            max_num_batched_tokens=1536,  # ë‹¨ì¼ ìš”ì²­ ì¤‘ì‹¬ ë°°ì¹˜
            max_num_seqs=8,               # ì ì€ ë™ì‹œ ì‚¬ìš©ì (8ëª…)
            served_model_name="Meow-HyperCLOVAX-LoRA"
        ),
        "haebo/Meow-HyperCLOVAX-1.5B_FullFT_fp32_0619i": ModelConfig(
            model_type=ModelType.FULL_FINETUNED,
            model_path="haebo/Meow-HyperCLOVAX-1.5B_FullFT_fp32_0619i",
            base_model_path=None,
            lora_modules=None,
            gpu_memory_utilization=0.5,   # ë‚®ì€ ë©”ëª¨ë¦¬ 
            max_model_len=1536,           # í•œêµ­ì–´ 600ì ì²˜ë¦¬ 
            max_num_batched_tokens=1536,  # ë‹¨ì¼ ìš”ì²­ ì¤‘ì‹¬ ë°°ì¹˜
            max_num_seqs=12,              # í’€ íŒŒì¸íŠœë‹ ëª¨ë¸ì€ ì•½ê°„ ë” ë§ì€ ë™ì‹œ ì²˜ë¦¬
            served_model_name="Meow-HyperCLOVAX-FullFT"
        )
    }


def detect_active_model_from_server() -> Optional[str]:
    """ì‹¤í–‰ ì¤‘ì¸ vLLM ì„œë²„ì—ì„œ í™œì„± ëª¨ë¸ ê°ì§€"""
    try:
        response = httpx.get("http://localhost:8001/v1/models", timeout=5.0)
        if response.status_code == 200:
            models_data = response.json()
            if models_data.get("data"):
                # ì²« ë²ˆì§¸ ëª¨ë¸ì˜ root ì •ë³´ì—ì„œ ì‹¤ì œ ëª¨ë¸ ê²½ë¡œ ì¶”ì¶œ
                model_info = models_data["data"][0]
                model_root = model_info.get("root", "")
                served_model_name = model_info.get("id", "")
                
                logger.info(f"ğŸ” ì„œë²„ì—ì„œ ê°ì§€ëœ ëª¨ë¸: {model_root} (ì„œë¹™ëª…: {served_model_name})")
                
                # ì§€ì› ëª¨ë¸ ëª©ë¡ì—ì„œ ë§¤ì¹­ë˜ëŠ” ëª¨ë¸ ì°¾ê¸°
                default_models = get_default_models()
                for model_name, config in default_models.items():
                    if (model_root == config.model_path or 
                        model_root == config.base_model_path or
                        served_model_name == config.served_model_name):
                        logger.info(f"âœ… ë§¤ì¹­ëœ ëª¨ë¸: {model_name}")
                        return model_name
                
                logger.warning(f"âš ï¸ ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_root}")
                return None
    except Exception as e:
        logger.debug(f"ì„œë²„ ëª¨ë¸ ê°ì§€ ì‹¤íŒ¨: {e}")
        return None
    
    return None


class VLLMConfig(BaseSettings):
    """vLLM ì„œë²„ ì„¤ì • í´ë˜ìŠ¤ - ë‹¤ì¤‘ ëª¨ë¸ íƒ€ì… ì§€ì›"""
    
    # ì„œë²„ ê¸°ë³¸ ì„¤ì •
    host: str = Field(default="0.0.0.0", description="vLLM ì„œë²„ í˜¸ìŠ¤íŠ¸")
    port: int = Field(default=8001, description="vLLM ì„œë²„ í¬íŠ¸")
    
    # ì§€ì› ëª¨ë¸ ì„¤ì • ë§µ (ë¨¼ì € ì´ˆê¸°í™”)
    supported_models: Dict[str, ModelConfig] = Field(
        default_factory=get_default_models,
        description="ì§€ì›í•˜ëŠ” ëª¨ë¸ë“¤ì˜ ì„¤ì •"
    )
    
    # í˜„ì¬ í™œì„± ëª¨ë¸ ì„¤ì • - ë™ì ìœ¼ë¡œ ê°ì§€
    active_model: str = Field(
        default="",  # ë¹ˆ ë¬¸ìì—´ë¡œ ì´ˆê¸°í™”
        description="í˜„ì¬ ì‚¬ìš©í•  ëª¨ë¸ ì´ë¦„"
    )
    
    # í† í°í™” ì„¤ì •
    trust_remote_code: bool = Field(default=True, description="ì›ê²© ì½”ë“œ ì‹ ë¢°")
    
    def __init__(self, **data):
        """ì´ˆê¸°í™” ì‹œ active_model ë™ì  ê°ì§€"""
        super().__init__(**data)
        
        # 1. ë¨¼ì € ì‹¤í–‰ ì¤‘ì¸ ì„œë²„ì—ì„œ ëª¨ë¸ ê°ì§€ ì‹œë„
        if not self.active_model:
            detected_model = detect_active_model_from_server()
            if detected_model and detected_model in self.supported_models:
                self.active_model = detected_model
                logger.info(f"ğŸ¯ ì„œë²„ì—ì„œ ìë™ ê°ì§€ëœ ëª¨ë¸: {self.active_model}")
        
        # 2. ê°ì§€ ì‹¤íŒ¨ ì‹œ í™˜ê²½ ë³€ìˆ˜ í™•ì¸
        if not self.active_model:
            env_model = os.getenv("VLLM_ACTIVE_MODEL")
            if env_model and env_model in self.supported_models:
                self.active_model = env_model
                logger.info(f"ğŸŒ í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì •ëœ ëª¨ë¸: {self.active_model}")
        
        # 3. ì—¬ì „íˆ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©
        if not self.active_model:
            available_models = list(self.supported_models.keys())
            if available_models:
                # í’€ íŒŒì¸íŠœë‹ ëª¨ë¸ ìš°ì„ , ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ëª¨ë¸
                fullft_models = [m for m in available_models if "FullFT" in m]
                self.active_model = fullft_models[0] if fullft_models else available_models[0]
                logger.info(f"ğŸ”§ ê¸°ë³¸ ëª¨ë¸ë¡œ ì„¤ì •: {self.active_model}")
            else:
                raise ValueError("ì§€ì›ë˜ëŠ” ëª¨ë¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        
        # ìµœì¢… ê²€ì¦
        if self.active_model not in self.supported_models:
            available_models = list(self.supported_models.keys())
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸: {self.active_model}. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {available_models}")
    
    def get_current_model_config(self) -> ModelConfig:
        """í˜„ì¬ í™œì„± ëª¨ë¸ì˜ ì„¤ì • ë°˜í™˜"""
        return self.supported_models[self.active_model]
    
    def is_lora_model(self) -> bool:
        """í˜„ì¬ ëª¨ë¸ì´ LoRA ëª¨ë¸ì¸ì§€ í™•ì¸"""
        return self.get_current_model_config().model_type == ModelType.LORA
    
    def is_full_finetuned_model(self) -> bool:
        """í˜„ì¬ ëª¨ë¸ì´ í’€ íŒŒì¸íŠœë‹ ëª¨ë¸ì¸ì§€ í™•ì¸"""
        return self.get_current_model_config().model_type == ModelType.FULL_FINETUNED
    
    
    # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
    class Config:
        env_prefix = "VLLM_"
        env_file = None
        case_sensitive = False
        extra = "ignore"


class VLLMServerArgs:
    """vLLM ì„œë²„ ì‹¤í–‰ ì¸ì ìƒì„± - ëª¨ë¸ íƒ€ì…ë³„ ìµœì í™”"""
    
    def __init__(self, config: VLLMConfig):
        self.config = config
        self.model_config = config.get_current_model_config()
    
    def get_server_args(self) -> list[str]:
        """ëª¨ë¸ íƒ€ì…ì— ë”°ë¥¸ ìµœì í™”ëœ vLLM ì„œë²„ ì‹¤í–‰ ì¸ì"""
        if self.model_config.model_type == ModelType.LORA:
            return self._get_lora_args()
        elif self.model_config.model_type == ModelType.FULL_FINETUNED:
            return self._get_full_finetuned_args()
        else:
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {self.model_config.model_type}")
    
    def _get_lora_args(self) -> list[str]:
        """LoRA ëª¨ë¸ìš© ì„œë²„ ì¸ì"""
        args = [
            "--host", self.config.host,
            "--port", str(self.config.port),
            "--model", self.model_config.base_model_path,
            "--served-model-name", self.model_config.served_model_name,
            "--gpu-memory-utilization", str(self.model_config.gpu_memory_utilization),
            "--max-model-len", str(self.model_config.max_model_len),
            "--max-num-batched-tokens", str(self.model_config.max_num_batched_tokens),
            "--max-num-seqs", str(self.model_config.max_num_seqs),
            "--trust-remote-code",
            "--enable-lora",
            "--max-loras", "1",
            "--max-lora-rank", "16",
        ]
        
        # LoRA ëª¨ë“ˆ ì¶”ê°€
        if self.model_config.lora_modules:
            for lora_module in self.model_config.lora_modules:
                args.extend(["--lora-modules", lora_module])
        
        return args
    
    def _get_full_finetuned_args(self) -> list[str]:
        """í’€ íŒŒì¸íŠœë‹ ëª¨ë¸ìš© ì„œë²„ ì¸ì"""
        args = [
            "--host", self.config.host,
            "--port", str(self.config.port),
            "--model", self.model_config.model_path,
            "--served-model-name", self.model_config.served_model_name,
            "--gpu-memory-utilization", str(self.model_config.gpu_memory_utilization),
            "--max-model-len", str(self.model_config.max_model_len),
            "--max-num-batched-tokens", str(self.model_config.max_num_batched_tokens),
            "--max-num-seqs", str(self.model_config.max_num_seqs),
            "--trust-remote-code",
            # í’€ íŒŒì¸íŠœë‹ ëª¨ë¸ ìµœì í™” ì˜µì…˜
            "--enforce-eager",  # ë©”ëª¨ë¦¬ ìµœì í™”
            "--disable-custom-all-reduce",  # ì•ˆì •ì„± í–¥ìƒ
        ]
        
        return args


# ì „ì—­ ì„¤ì • ì¸ìŠ¤í„´ìŠ¤
vllm_config = VLLMConfig()


def get_vllm_config() -> VLLMConfig:
    """vLLM ì„¤ì • ì¸ìŠ¤í„´ìŠ¤ ë°˜í™˜"""
    return vllm_config


def update_vllm_config(**kwargs) -> VLLMConfig:
    """vLLM ì„¤ì • ì—…ë°ì´íŠ¸"""
    global vllm_config
    for key, value in kwargs.items():
        if hasattr(vllm_config, key):
            setattr(vllm_config, key, value)
    return vllm_config


def switch_model(model_name: str) -> VLLMConfig:
    """í™œì„± ëª¨ë¸ ì „í™˜"""
    global vllm_config
        if not vllm_config.validate_active_model(model_name):
        available_models = list(vllm_config.supported_models.keys())
        raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_name}. ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸: {available_models}")
    
    vllm_config.active_model = model_name
    return vllm_config 