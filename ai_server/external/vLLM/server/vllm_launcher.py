"""
vLLM ì„œë²„ ëŸ°ì²˜
ë‹¤ì¤‘ ëª¨ë¸ íƒ€ì… ì§€ì› (LoRA, í’€ íŒŒì¸íŠœë‹)
"""

import os
import sys
import subprocess
import signal
import time
import logging
import psutil
from typing import Optional, List
from pathlib import Path
import requests
from huggingface_hub import HfApi
from huggingface_hub.utils import RepositoryNotFoundError

from ai_server.external.vLLM.server.vllm_config import VLLMConfig, VLLMServerArgs, get_vllm_config, ModelType


# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


class ModelDetector:
    """ëª¨ë¸ íƒ€ì… ìë™ ê°ì§€ í´ë˜ìŠ¤"""
    
    @staticmethod
    def detect_model_type(model_path: str) -> ModelType:
        """ëª¨ë¸ ê²½ë¡œë¡œë¶€í„° ëª¨ë¸ íƒ€ì… ê°ì§€"""
        try:
            # í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ì¸ ê²½ìš°
            if "/" in model_path and not model_path.startswith(("./", "/")):
                return ModelDetector._detect_hf_model_type(model_path)
            
            # ë¡œì»¬ ëª¨ë¸ì¸ ê²½ìš°
            return ModelDetector._detect_local_model_type(model_path)
            
        except Exception as e:
            logger.warning(f"ëª¨ë¸ íƒ€ì… ê°ì§€ ì‹¤íŒ¨: {e}")
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_path}")
    
    @staticmethod
    def _detect_hf_model_type(model_path: str) -> ModelType:
        """í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ íƒ€ì… ê°ì§€"""
        try:
            api = HfApi()
            
            # ëª¨ë¸ ì •ë³´ ì¡°íšŒ
            model_info = api.model_info(model_path)
            
            # íƒœê·¸ ê¸°ë°˜ ê°ì§€
            if model_info.tags:
                tags = [tag.lower() for tag in model_info.tags]
                if "lora" in tags or "peft" in tags:
                    return ModelType.LORA
                elif "fine-tuned" in tags or "finetuned" in tags:
                    return ModelType.FULL_FINETUNED
            
            # íŒŒì¼ êµ¬ì¡° ê¸°ë°˜ ê°ì§€
            try:
                files = api.list_repo_files(model_path)
                
                # LoRA ê´€ë ¨ íŒŒì¼ í™•ì¸
                lora_files = [
                    "adapter_config.json", 
                    "adapter_model.bin", 
                    "adapter_model.safetensors",
                    "peft_config.json"
                ]
                
                if any(f in files for f in lora_files):
                    return ModelType.LORA
                
                # í’€ ëª¨ë¸ íŒŒì¼ í™•ì¸
                full_model_files = [
                    "pytorch_model.bin",
                    "model.safetensors", 
                    "pytorch_model-00001-of-*.bin"
                ]
                
                if any(any(f.startswith(pattern.split('*')[0]) for f in files) 
                       for pattern in full_model_files):
                    return ModelType.FULL_FINETUNED
                    
            except Exception as e:
                logger.debug(f"íŒŒì¼ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            
            # ëª¨ë¸ëª… ê¸°ë°˜ ì¶”ë¡ 
            model_name_lower = model_path.lower()
            if "lora" in model_name_lower or "peft" in model_name_lower:
                return ModelType.LORA
            elif "ft" in model_name_lower or "finetun" in model_name_lower:
                return ModelType.FULL_FINETUNED
                
            raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_path}")
            
        except RepositoryNotFoundError:
            logger.error(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
            raise ValueError(f"ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        except Exception as e:
            logger.error(f"í—ˆê¹…í˜ì´ìŠ¤ ëª¨ë¸ ê°ì§€ ì‹¤íŒ¨: {e}")
            raise ValueError(f"ëª¨ë¸ ê°ì§€ ì‹¤íŒ¨: {model_path}")
    
    @staticmethod
    def _detect_local_model_type(model_path: str) -> ModelType:
        """ë¡œì»¬ ëª¨ë¸ íƒ€ì… ê°ì§€"""
        path = Path(model_path)
        
        if not path.exists():
            logger.warning(f"ë¡œì»¬ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
            raise ValueError(f"ë¡œì»¬ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
        
        # LoRA ê´€ë ¨ íŒŒì¼ í™•ì¸
        lora_files = [
            "adapter_config.json",
            "adapter_model.bin", 
            "adapter_model.safetensors",
            "peft_config.json"
        ]
        
        if any((path / f).exists() for f in lora_files):
            return ModelType.LORA
        
        # í’€ ëª¨ë¸ íŒŒì¼ í™•ì¸
        full_model_files = [
            "pytorch_model.bin",
            "model.safetensors"
        ]
        
        if any((path / f).exists() for f in full_model_files):
            return ModelType.FULL_FINETUNED
        
        raise ValueError(f"ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {model_path}")


class VLLMLauncher:
    """vLLM ì„œë²„ ëŸ°ì²˜ í´ë˜ìŠ¤ - ë‹¤ì¤‘ ëª¨ë¸ íƒ€ì… ì§€ì›"""
    
    def __init__(self, config: Optional[VLLMConfig] = None):
        self.config = config or get_vllm_config()
        self.process: Optional[subprocess.Popen] = None
        self.server_args = VLLMServerArgs(self.config)
        self.detector = ModelDetector()
        
        # ì‹œì‘ ì‹œ ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ìë™ ì—°ê²° ì‹œë„
        self._try_reconnect_existing_process()
    
    def _try_reconnect_existing_process(self) -> bool:
        """ê¸°ì¡´ ì‹¤í–‰ ì¤‘ì¸ vLLM í”„ë¡œì„¸ìŠ¤ ìë™ ì—°ê²°"""
        try:
            vllm_pids = self._find_vllm_processes()
            if vllm_pids:
                # ê°€ì¥ ìµœê·¼ í”„ë¡œì„¸ìŠ¤ì— ì—°ê²°
                target_pid = max(vllm_pids)
                if self._connect_to_existing_process(target_pid):
                    logger.info(f"ğŸ”— ê¸°ì¡´ vLLM í”„ë¡œì„¸ìŠ¤ì— ì—°ê²°ë¨: PID {target_pid}")
                    return True
        except Exception as e:
            logger.debug(f"ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {e}")
        return False
    
    def _find_vllm_processes(self) -> List[int]:
        """ì‹¤í–‰ ì¤‘ì¸ vLLM ì„œë²„ í”„ë¡œì„¸ìŠ¤ ì°¾ê¸°"""
        vllm_pids = []
        try:
            for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                try:
                    cmdline = proc.info['cmdline']
                    if (cmdline and 
                        any('vllm.entrypoints.openai.api_server' in str(cmd) for cmd in cmdline)):
                        # í¬íŠ¸ë„ í™•ì¸
                        if f"--port {self.config.port}" in ' '.join(cmdline) or str(self.config.port) in cmdline:
                            vllm_pids.append(proc.info['pid'])
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    continue
        except Exception as e:
            logger.debug(f"í”„ë¡œì„¸ìŠ¤ ê²€ìƒ‰ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return vllm_pids
    
    def _connect_to_existing_process(self, pid: int) -> bool:
        """ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ì— ì—°ê²°"""
        try:
            # PIDë¡œ í”„ë¡œì„¸ìŠ¤ ê°ì²´ ìƒì„± (ì¶”ì  ëª©ì )
            proc = psutil.Process(pid)
            if proc.is_running() and proc.status() != psutil.STATUS_ZOMBIE:
                # subprocess.Popen ê°ì²´ëŠ” ë§Œë“¤ ìˆ˜ ì—†ì§€ë§Œ, PIDëŠ” ì €ì¥
                self.process = type('MockProcess', (), {
                    'pid': pid,
                    'poll': lambda *args, **kwargs: None if proc.is_running() else 0,
                    'wait': lambda *args, **kwargs: proc.wait(timeout=kwargs.get('timeout')),
                    'terminate': lambda *args, **kwargs: proc.terminate(),
                    'kill': lambda *args, **kwargs: proc.kill()
                })()
                return True
        except (psutil.NoSuchProcess, psutil.AccessDenied, Exception) as e:
            logger.debug(f"í”„ë¡œì„¸ìŠ¤ ì—°ê²° ì‹¤íŒ¨ (PID: {pid}): {e}")
        return False
    
    def is_running(self) -> bool:
        """vLLM ì„œë²„ ì‹¤í–‰ ìƒíƒœ í™•ì¸ (ë‹¤ì¤‘ ë°©ë²•)"""
        # ë°©ë²• 1: í”„ë¡œì„¸ìŠ¤ ê°ì²´ ê¸°ë°˜ í™•ì¸
        if self.process:
            try:
                # Mock í”„ë¡œì„¸ìŠ¤ì¸ ê²½ìš° poll() ë©”ì„œë“œ ì‚¬ìš©
                if hasattr(self.process, 'poll') and callable(self.process.poll):
                    return self.process.poll() is None
                # ì‹¤ì œ subprocess.Popenì¸ ê²½ìš°
                elif hasattr(self.process, 'pid'):
                    return psutil.pid_exists(self.process.pid)
            except Exception as e:
                logger.debug(f"í”„ë¡œì„¸ìŠ¤ ê°ì²´ í™•ì¸ ì‹¤íŒ¨: {e}")
        
        # ë°©ë²• 2: PID ê¸°ë°˜ í”„ë¡œì„¸ìŠ¤ ì¡´ì¬ í™•ì¸
        if self._check_vllm_process_exists():
            # ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì¬ì—°ê²° ì‹œë„
            self._try_reconnect_existing_process()
            return True
        
        # ë°©ë²• 3: í¬íŠ¸ ê¸°ë°˜ HTTP í—¬ìŠ¤ì²´í¬
        return self._check_server_health()
    
    def _check_vllm_process_exists(self) -> bool:
        """PID ê¸°ë°˜ vLLM í”„ë¡œì„¸ìŠ¤ ì¡´ì¬ í™•ì¸"""
        vllm_pids = self._find_vllm_processes()
        return len(vllm_pids) > 0
    
    def _check_server_health(self) -> bool:
        """HTTP ìš”ì²­ìœ¼ë¡œ ì„œë²„ ìƒíƒœ í™•ì¸"""
        try:
            url = f"http://{self.config.host}:{self.config.port}/health"
            response = requests.get(url, timeout=3)
            return response.status_code == 200
        except requests.exceptions.RequestException:
            pass
        
        # /health ì—”ë“œí¬ì¸íŠ¸ê°€ ì—†ëŠ” ê²½ìš° /v1/models ì‹œë„
        try:
            url = f"http://{self.config.host}:{self.config.port}/v1/models"
            response = requests.get(url, timeout=3)
            return response.status_code == 200
        except requests.exceptions.RequestException:
            return False
    
    def get_process_info(self) -> Optional[dict]:
        """ì‹¤í–‰ ì¤‘ì¸ vLLM í”„ë¡œì„¸ìŠ¤ ìƒì„¸ ì •ë³´"""
        try:
            vllm_pids = self._find_vllm_processes()
            if not vllm_pids:
                return None
            
            # ê°€ì¥ ìµœê·¼ í”„ë¡œì„¸ìŠ¤ ì •ë³´ ë°˜í™˜
            target_pid = max(vllm_pids)
            proc = psutil.Process(target_pid)
            
            return {
                "pid": target_pid,
                "status": proc.status(),
                "cpu_percent": proc.cpu_percent(),
                "memory_percent": proc.memory_percent(),
                "create_time": proc.create_time(),
                "cmdline": proc.cmdline(),
                "connections": [
                    {"laddr": conn.laddr, "status": conn.status}
                    for conn in proc.connections()
                    if conn.laddr.port == self.config.port
                ]
            }
        except Exception as e:
            logger.debug(f"í”„ë¡œì„¸ìŠ¤ ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None
    
    def start_server(self) -> bool:
        """vLLM ì„œë²„ ì‹œì‘ - ëª¨ë¸ íƒ€ì…ë³„ ìµœì í™”"""
        if self.process and self.process.poll() is None:
            logger.warning("vLLM ì„œë²„ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            return True
        
        # ì„œë²„ ì‹¤í–‰ ëª…ë ¹ êµ¬ì„±
        cmd = ["python", "-m", "vllm.entrypoints.openai.api_server"] + self.server_args.get_server_args()
        
        model_config = self.config.get_current_model_config()
        logger.info(f"{model_config.model_type.value} ëª¨ë¸ ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        logger.info(f"ëª¨ë¸: {self.config.active_model}")
        logger.info(f"ì‹¤í–‰ ëª…ë ¹: {' '.join(cmd)}")
        
        try:
            # í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
            env = os.environ.copy()
            env.update({
                "CUDA_VISIBLE_DEVICES": "0",
                "VLLM_WORKER_MULTIPROC_METHOD": "spawn",  # ì•ˆì •ì„± í–¥ìƒ
            })
            
            self.process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                env=env,
                preexec_fn=os.setsid
            )
            
            logger.info(f"vLLM ì„œë²„ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. PID: {self.process.pid}")
            return self._wait_for_server_ready()
            
        except Exception as e:
            logger.error(f"vLLM ì„œë²„ ì‹œì‘ ì‹¤íŒ¨: {e}")
            return False
    
    def _wait_for_server_ready(self, timeout: int = 300) -> bool:
        """ì„œë²„ ì¤€ë¹„ ìƒíƒœ ëŒ€ê¸° - ëª¨ë¸ íƒ€ì…ë³„ ì¡°ì •ëœ íƒ€ì„ì•„ì›ƒ"""
        url = f"http://{self.config.host}:{self.config.port}/v1/models"
        start_time = time.time()
        
        model_config = self.config.get_current_model_config()
        
        # ëª¨ë¸ íƒ€ì…ë³„ íƒ€ì„ì•„ì›ƒ ì¡°ì •
        if model_config.model_type == ModelType.LORA:
            timeout = 240  # LoRAëŠ” ë² ì´ìŠ¤ ëª¨ë¸ + ì–´ëŒ‘í„° ë¡œë”© ì‹œê°„
        elif model_config.model_type == ModelType.FULL_FINETUNED:
            timeout = 360  # í’€ íŒŒì¸íŠœë‹ ëª¨ë¸ì€ ë” ê¸´ ë¡œë”© ì‹œê°„
        
        logger.info(f"ì„œë²„ ì¤€ë¹„ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤... (ìµœëŒ€ {timeout}ì´ˆ)")
        
        while time.time() - start_time < timeout:
            if self.process and self.process.poll() is not None:
                # í”„ë¡œì„¸ìŠ¤ ì˜¤ë¥˜ ë¡œê·¸ ì¶œë ¥
                stdout, stderr = self.process.communicate()
                logger.error("vLLM ì„œë²„ í”„ë¡œì„¸ìŠ¤ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
                if stderr:
                    logger.error(f"ì˜¤ë¥˜ ë¡œê·¸: {stderr.decode()}")
                return False
            
            try:
                response = requests.get(url, timeout=5)
                if response.status_code == 200:
                    logger.info(f"{model_config.model_type.value} ëª¨ë¸ ì„œë²„ê°€ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    logger.info(f"ì„œë¹™ ëª¨ë¸ëª…: {model_config.served_model_name}")
                    return True
            except requests.exceptions.RequestException:
                pass
            
            time.sleep(3)
        
        logger.error(f"ì„œë²„ ì¤€ë¹„ ëŒ€ê¸° ì‹œê°„ ì´ˆê³¼ ({timeout}ì´ˆ)")
        return False
    
    def stop_server(self) -> bool:
        """vLLM ì„œë²„ ì¤‘ì§€ - ê°œì„ ëœ í”„ë¡œì„¸ìŠ¤ ì²˜ë¦¬"""
        # í”„ë¡œì„¸ìŠ¤ ê°ì²´ê°€ ì—†ëŠ” ê²½ìš°, ì‹¤í–‰ ì¤‘ì¸ vLLM í”„ë¡œì„¸ìŠ¤ ì°¾ì•„ì„œ ì¤‘ì§€
        if not self.process:
            vllm_pids = self._find_vllm_processes()
            if not vllm_pids:
                logger.info("ì‹¤í–‰ ì¤‘ì¸ vLLM ì„œë²„ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return True
            
            # ëª¨ë“  vLLM í”„ë¡œì„¸ìŠ¤ ì¤‘ì§€
            for pid in vllm_pids:
                try:
                    proc = psutil.Process(pid)
                    logger.info(f"ë°œê²¬ëœ vLLM í”„ë¡œì„¸ìŠ¤ ì¤‘ì§€ ì¤‘: PID {pid}")
                    proc.terminate()
                    proc.wait(timeout=30)
                    logger.info(f"vLLM í”„ë¡œì„¸ìŠ¤ê°€ ì •ìƒì ìœ¼ë¡œ ì¤‘ì§€ë¨: PID {pid}")
                except psutil.TimeoutExpired:
                    logger.warning(f"ì •ìƒ ì¢…ë£Œ ì‹œê°„ ì´ˆê³¼. ê°•ì œ ì¢…ë£Œ: PID {pid}")
                    proc.kill()
                except Exception as e:
                    logger.error(f"í”„ë¡œì„¸ìŠ¤ ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜ (PID {pid}): {e}")
            return True
        
        # í”„ë¡œì„¸ìŠ¤ ìƒíƒœ í™•ì¸
        try:
            if hasattr(self.process, 'poll') and callable(self.process.poll):
                if self.process.poll() is not None:
                    logger.info("vLLM ì„œë²„ê°€ ì´ë¯¸ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    self.process = None
                    return True
        except Exception:
            pass
        
        logger.info("vLLM ì„œë²„ë¥¼ ì¤‘ì§€í•©ë‹ˆë‹¤...")
        
        try:
            # Mock í”„ë¡œì„¸ìŠ¤ì¸ ê²½ìš°
            if hasattr(self.process, 'terminate') and not hasattr(self.process, 'communicate'):
                try:
                    self.process.terminate()
                    if hasattr(self.process, 'wait'):
                        self.process.wait(timeout=30)
                    logger.info("vLLM ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    return True
                except Exception as e:
                    logger.warning(f"ì •ìƒ ì¢…ë£Œ ì‹¤íŒ¨, ê°•ì œ ì¢…ë£Œ ì‹œë„: {e}")
                    if hasattr(self.process, 'kill'):
                        self.process.kill()
                    return True
            
            # ì‹¤ì œ subprocess.Popenì¸ ê²½ìš°
            else:
                os.killpg(os.getpgid(self.process.pid), signal.SIGTERM)
                
                try:
                    self.process.wait(timeout=30)
                    logger.info("vLLM ì„œë²„ê°€ ì •ìƒì ìœ¼ë¡œ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    return True
                except subprocess.TimeoutExpired:
                    logger.warning("ì •ìƒ ì¢…ë£Œ ì‹œê°„ ì´ˆê³¼. ê°•ì œ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                    os.killpg(os.getpgid(self.process.pid), signal.SIGKILL)
                    self.process.wait()
                    return True
                    
        except Exception as e:
            logger.error(f"ì„œë²„ ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False
        finally:
            self.process = None
    
    def restart_server(self) -> bool:
        """vLLM ì„œë²„ ì¬ì‹œì‘"""
        logger.info("vLLM ì„œë²„ë¥¼ ì¬ì‹œì‘í•©ë‹ˆë‹¤...")
        self.stop_server()
        time.sleep(5)
        return self.start_server()
    
    def get_server_status(self) -> dict:
        """ê°œì„ ëœ ì„œë²„ ìƒíƒœ ì •ë³´ ë°˜í™˜"""
        model_config = self.config.get_current_model_config()
        
        # ê¸°ë³¸ ìƒíƒœ ì •ë³´
        is_running = self.is_running()
        process_info = self.get_process_info() if is_running else None
        
        status = {
            "running": is_running,
            "pid": process_info["pid"] if process_info else (self.process.pid if self.process else None),
            "active_model": self.config.active_model,
            "model_type": model_config.model_type.value,
            "served_model_name": model_config.served_model_name,
            "host": self.config.host,
            "port": self.config.port,
            
            # ì¶”ê°€ ìƒíƒœ ì •ë³´
            "health_check": {
                "process_exists": self._check_vllm_process_exists(),
                "http_accessible": self._check_server_health(),
                "process_tracked": self.process is not None
            }
        }
        
        # í”„ë¡œì„¸ìŠ¤ ìƒì„¸ ì •ë³´ ì¶”ê°€
        if process_info:
            status.update({
                "process_status": process_info["status"],
                "cpu_percent": process_info["cpu_percent"],
                "memory_percent": process_info["memory_percent"],
                "uptime": time.time() - process_info["create_time"]
            })
        
        return status


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ë‹¤ì¤‘ ëª¨ë¸ íƒ€ì… ì§€ì› vLLM ì„œë²„ ëŸ°ì²˜")
    parser.add_argument("--action", choices=["start", "stop", "restart", "status"], 
                       default="start", help="ì‹¤í–‰í•  ì•¡ì…˜")
    parser.add_argument("--port", type=int, help="ì„œë²„ í¬íŠ¸")
    
    args = parser.parse_args()
    
    launcher = VLLMLauncher()
    
    if args.action == "start":
        success = launcher.start_server()
        sys.exit(0 if success else 1)
    elif args.action == "stop":
        success = launcher.stop_server()
        sys.exit(0 if success else 1)
    elif args.action == "restart":
        success = launcher.restart_server()
        sys.exit(0 if success else 1)
    elif args.action == "status":
        status = launcher.get_server_status()
        print("=== vLLM ì„œë²„ ìƒíƒœ ===")
        for key, value in status.items():
            print(f"{key}: {value}")
        sys.exit(0)


if __name__ == "__main__":
    main() 