#!/usr/bin/env python3
"""
vLLM ëª¨ë¸ ê´€ë¦¬ ë„êµ¬
LoRAì™€ í’€ íŒŒì¸íŠœë‹ ëª¨ë¸ ê°„ ì „í™˜ ë° ê´€ë¦¬
"""

import sys
import os
import argparse
import json
from pathlib import Path
from typing import Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from ai_server.vLLM import (
    VLLMLauncher, 
    ModelDetector, 
    get_vllm_config, 
    switch_model,
    ModelType,
    ModelConfig
)


class ModelManager:
    """ëª¨ë¸ ê´€ë¦¬ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.launcher = VLLMLauncher()
        self.detector = ModelDetector()
        self.config = get_vllm_config()
    
    def list_models(self) -> None:
        """ì§€ì›ë˜ëŠ” ëª¨ë¸ ëª©ë¡ ì¶œë ¥"""
        print("=== ì§€ì›ë˜ëŠ” ëª¨ë¸ ëª©ë¡ ===")
        print(f"í˜„ì¬ í™œì„± ëª¨ë¸: {self.config.active_model}")
        print()
        
        for model_name, model_config in self.config.supported_models.items():
            status = "ğŸŸ¢ í™œì„±" if model_name == self.config.active_model else "âšª ë¹„í™œì„±"
            print(f"{status} {model_name}")
            print(f"  íƒ€ì…: {model_config.model_type.value}")
            print(f"  ê²½ë¡œ: {model_config.model_path}")
            if model_config.base_model_path:
                print(f"  ë² ì´ìŠ¤ ëª¨ë¸: {model_config.base_model_path}")
            print(f"  ì„œë¹™ëª…: {model_config.served_model_name}")
            print(f"  GPU ë©”ëª¨ë¦¬: {model_config.gpu_memory_utilization * 100:.0f}%")
            print()
    
    def switch_model(self, model_name: str) -> bool:
        """ëª¨ë¸ ì „í™˜ ë° ì„œë²„ ì‹œì‘"""
        try:
            print(f"ëª¨ë¸ì„ '{model_name}'ìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤...")
            
            if model_name not in self.config.supported_models:
                print(f"âŒ ì§€ì›ë˜ì§€ ì•ŠëŠ” ëª¨ë¸: {model_name}")
                print("ì§€ì›ë˜ëŠ” ëª¨ë¸ ëª©ë¡:")
                for name in self.config.supported_models.keys():
                    print(f"  - {name}")
                return False
            
            # ê¸°ì¡´ ì„œë²„ê°€ ì‹¤í–‰ ì¤‘ì´ë©´ ë¨¼ì € ì¤‘ì§€
            if self.launcher.process and self.launcher.process.poll() is None:
                print("ê¸°ì¡´ ì„œë²„ë¥¼ ì¤‘ì§€í•©ë‹ˆë‹¤...")
                self.launcher.stop_server()
            
            # ëª¨ë¸ ì„¤ì • ë³€ê²½
            from ai_server.vLLM.server.vllm_config import switch_model
            switch_model(model_name)
            
            # ìƒˆ ì„¤ì •ìœ¼ë¡œ launcher ì—…ë°ì´íŠ¸
            self.launcher = VLLMLauncher()
            
            print(f"âœ… ëª¨ë¸ ì„¤ì • ë³€ê²½ ì™„ë£Œ: {model_name}")
            print("ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            
            # ì„œë²„ ì‹œì‘
            success = self.launcher.start_server()
            
            if success:
                print(f"âœ… '{model_name}' ëª¨ë¸ë¡œ ì„œë²„ ì‹œì‘ ì™„ë£Œ")
                return True
            else:
                print(f"âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì „í™˜ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def start_server(self) -> bool:
        """ì„œë²„ ì‹œì‘"""
        try:
            print("vLLM ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            success = self.launcher.start_server()
            
            if success:
                print("âœ… ì„œë²„ ì‹œì‘ ì™„ë£Œ")
                self.show_status()
                return True
            else:
                print("âŒ ì„œë²„ ì‹œì‘ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            print(f"âŒ ì„œë²„ ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def stop_server(self) -> bool:
        """ì„œë²„ ì¤‘ì§€"""
        try:
            print("vLLM ì„œë²„ë¥¼ ì¤‘ì§€í•©ë‹ˆë‹¤...")
            success = self.launcher.stop_server()
            
            if success:
                print("âœ… ì„œë²„ ì¤‘ì§€ ì™„ë£Œ")
                return True
            else:
                print("âŒ ì„œë²„ ì¤‘ì§€ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            print(f"âŒ ì„œë²„ ì¤‘ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def restart_server(self) -> bool:
        """ì„œë²„ ì¬ì‹œì‘"""
        try:
            print("vLLM ì„œë²„ë¥¼ ì¬ì‹œì‘í•©ë‹ˆë‹¤...")
            success = self.launcher.restart_server()
            
            if success:
                print("âœ… ì„œë²„ ì¬ì‹œì‘ ì™„ë£Œ")
                self.show_status()
                return True
            else:
                print("âŒ ì„œë²„ ì¬ì‹œì‘ ì‹¤íŒ¨")
                return False
                
        except Exception as e:
            print(f"âŒ ì„œë²„ ì¬ì‹œì‘ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def show_status(self) -> None:
        """ì„œë²„ ìƒíƒœ ì¶œë ¥"""
        status = self.launcher.get_server_status()
        
        print("=== vLLM ì„œë²„ ìƒíƒœ ===")
        print(f"ì‹¤í–‰ ìƒíƒœ: {'ğŸŸ¢ ì‹¤í–‰ ì¤‘' if status['running'] else 'ğŸ”´ ì¤‘ì§€'}")
        
        if status['running']:
            print(f"PID: {status['pid']}")
            print(f"ì£¼ì†Œ: http://{status['host']}:{status['port']}")
        
        print(f"í™œì„± ëª¨ë¸: {status['active_model']}")
        print(f"ëª¨ë¸ íƒ€ì…: {status['model_type']}")
        print(f"ì„œë¹™ ëª¨ë¸ëª…: {status['served_model_name']}")
        print()
    
    def detect_model(self, model_path: str) -> None:
        """ëª¨ë¸ íƒ€ì… ê°ì§€"""
        try:
            detected_type = self.detector.detect_model_type(model_path)
            
            print(f"ê°ì§€ëœ ëª¨ë¸ íƒ€ì…: {detected_type.value}")
            
            # ê°ì§€ ê²°ê³¼ì— ë”°ë¥¸ ê¶Œì¥ ì„¤ì • ì¶œë ¥
            if detected_type == ModelType.LORA:
                print("\nğŸ“‹ LoRA ëª¨ë¸ ì„¤ì • ê°€ì´ë“œ:")
                print("- base_model_path: ë² ì´ìŠ¤ ëª¨ë¸ ê²½ë¡œ ì„¤ì • í•„ìš”")
                print("- lora_modules: LoRA ì–´ëŒ‘í„° ëª¨ë“ˆ ì„¤ì • í•„ìš”")
                print("- GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : 0.7 ê¶Œì¥")
            elif detected_type == ModelType.FULL_FINETUNED:
                print("\nğŸ“‹ í’€ íŒŒì¸íŠœë‹ ëª¨ë¸ ì„¤ì • ê°€ì´ë“œ:")
                print("- ë² ì´ìŠ¤ ëª¨ë¸ ê²½ë¡œ ë¶ˆí•„ìš”")
                print("- GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : 0.9 ê¶Œì¥")
                print("- ë” ë§ì€ ë°°ì¹˜ í¬ê¸° ë° ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì • ê°€ëŠ¥")
            else:
                print("\nğŸ“‹ ê¸°ë³¸ ëª¨ë¸ ì„¤ì • ê°€ì´ë“œ:")
                print("- í‘œì¤€ ì„¤ì • ì‚¬ìš©")
                print("- GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ : 0.8 ê¶Œì¥")
                
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ê°ì§€ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def export_config(self, output_path: str) -> bool:
        """í˜„ì¬ ì„¤ì •ì„ íŒŒì¼ë¡œ ë‚´ë³´ë‚´ê¸°"""
        try:
            config_dict = {
                "active_model": self.config.active_model,
                "host": self.config.host,
                "port": self.config.port,
                "supported_models": {}
            }
            
            # ëª¨ë¸ ì„¤ì • ì§ë ¬í™”
            for name, model_config in self.config.supported_models.items():
                config_dict["supported_models"][name] = {
                    "model_type": model_config.model_type.value,
                    "model_path": model_config.model_path,
                    "base_model_path": model_config.base_model_path,
                    "lora_modules": model_config.lora_modules,
                    "gpu_memory_utilization": model_config.gpu_memory_utilization,
                    "max_model_len": model_config.max_model_len,
                    "max_num_batched_tokens": model_config.max_num_batched_tokens,
                    "max_num_seqs": model_config.max_num_seqs,
                    "served_model_name": model_config.served_model_name
                }
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(config_dict, f, indent=2, ensure_ascii=False)
            
            print(f"âœ… ì„¤ì •ì´ '{output_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
            
        except Exception as e:
            print(f"âŒ ì„¤ì • ë‚´ë³´ë‚´ê¸° ì‹¤íŒ¨: {e}")
            return False


def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(
        description="vLLM ëª¨ë¸ ê´€ë¦¬ ë„êµ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‚¬ìš© ì˜ˆì‹œ:
  # ëª¨ë¸ ëª©ë¡ í™•ì¸
  python model_manager.py list
  
  # ëª¨ë¸ ì „í™˜ ë° ì„œë²„ ì‹œì‘
  python model_manager.py switch haebo/Meow-HyperCLOVAX-1.5B_FullFT_fp16_0619i
  
  # ì„œë²„ ìƒíƒœ í™•ì¸
  python model_manager.py status
  
  # ëª¨ë¸ íƒ€ì… ê°ì§€
  python model_manager.py detect haebo/Meow-HyperCLOVAX-1.5B_LoRA_fp16_0619i
  
  # ì„¤ì • ë‚´ë³´ë‚´ê¸°
  python model_manager.py export config.json
        """
    )
    
    subparsers = parser.add_subparsers(dest='command', help='ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹')
    
    # list ëª…ë ¹
    subparsers.add_parser('list', help='ì§€ì›ë˜ëŠ” ëª¨ë¸ ëª©ë¡ ì¶œë ¥')
    
    # switch ëª…ë ¹
    switch_parser = subparsers.add_parser('switch', help='ëª¨ë¸ ì „í™˜')
    switch_parser.add_argument('model_name', help='ì „í™˜í•  ëª¨ë¸ ì´ë¦„')
    switch_parser.add_argument('--no-start', action='store_true', help='ì„œë²„ ì‹œì‘í•˜ì§€ ì•ŠìŒ')
    
    # start ëª…ë ¹
    subparsers.add_parser('start', help='ì„œë²„ ì‹œì‘')
    
    # stop ëª…ë ¹
    subparsers.add_parser('stop', help='ì„œë²„ ì¤‘ì§€')
    
    # restart ëª…ë ¹
    subparsers.add_parser('restart', help='ì„œë²„ ì¬ì‹œì‘')
    
    # status ëª…ë ¹
    subparsers.add_parser('status', help='ì„œë²„ ìƒíƒœ í™•ì¸')
    
    # detect ëª…ë ¹
    detect_parser = subparsers.add_parser('detect', help='ëª¨ë¸ íƒ€ì… ê°ì§€')
    detect_parser.add_argument('model_path', help='ê°ì§€í•  ëª¨ë¸ ê²½ë¡œ')
    
    # export ëª…ë ¹
    export_parser = subparsers.add_parser('export', help='ì„¤ì • ë‚´ë³´ë‚´ê¸°')
    export_parser.add_argument('output_path', help='ì¶œë ¥ íŒŒì¼ ê²½ë¡œ')
    
    args = parser.parse_args()
    
    if not args.command:
        parser.print_help()
        return
    
    manager = ModelManager()
    
    try:
        if args.command == 'list':
            manager.list_models()
        
        elif args.command == 'switch':
            success = manager.switch_model(args.model_name)
            if success and not args.no_start:
                manager.start_server()
        
        elif args.command == 'start':
            manager.start_server()
        
        elif args.command == 'stop':
            manager.stop_server()
        
        elif args.command == 'restart':
            manager.restart_server()
        
        elif args.command == 'status':
            manager.show_status()
        
        elif args.command == 'detect':
            manager.detect_model(args.model_path)
        
        elif args.command == 'export':
            manager.export_config(args.output_path)
        
    except KeyboardInterrupt:
        print("\nì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main() 