name: LLM API CI

on:
  push:
    branches:
      - 'feature/**'
      - '*-feat-*'
  pull_request:
    branches:
      - 'dev'
      - 'release'

jobs:
  build-and-test:
    if: >
      (github.event_name == 'push' && (startsWith(github.ref, 'refs/heads/feature/') || contains(github.ref, 'feat-')))
      || (github.event_name == 'pull_request' && github.base_ref == 'dev')
    name: ðŸ¤– Build & Lint
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3

      - name: Setup Python 3.9
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install deps
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Lint / Syntax Check
        run: python -m pyflakes ai_server

  integration:
    name: ðŸ”— AI Server Integration Tests
    runs-on: ubuntu-latest
    needs: build-and-test

    steps:
      - uses: actions/checkout@v3

      - name: Setup Python 3.9
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install deps
        run: |
          pip install --upgrade pip
          pip install -r requirements.txt

      - name: Start FastAPI
        run: uvicorn ai_server.main:app --host 0.0.0.0 --port 8000 &

      - name: Wait for Health (root)
        run: |
          for i in {1..10}; do
            if curl -s http://localhost:8000/; then
              echo "âœ… AI Server (/) is up"
              exit 0
            fi
            sleep 5
          done
          exit 1